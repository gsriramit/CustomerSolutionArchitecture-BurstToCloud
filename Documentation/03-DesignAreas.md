# Design Areas

## 1. Application Platform Design
### Global distribution of platform resources  
**Excerpt from the official doc**: The mission-critical design methodology requires a multi-region deployment. This model ensures regional fault tolerance, so that the application remains available even when an entire region goes down.  
In this solution, we have considered an Active-Hot-Standy deployment model. This design can help us handle the business case of the Azure stamp(s) having have to handle the excessive traffic alone and need not be a full blown deployment.The global resources in this architecture include the Traffic Manager (global load balancer), Logic App, storage account and Azure monitoring solutions incuding a Log analytics workspace and an app-insights instance. As opposed to the guidance in the Mission-Critical architecture, the database that stores the application's state cannot be a global resource. This and a few other deviations from the Mission-Critical standards have been explained in the appropriate sections  
#### Design Considerations & Recommendations
1. **Regional and Zonal Capabilities**-  The proposed architecture comes with the following suggestions 
   - Choose an Azure region (for the burst tarffic) within the same geography
   - The region needs to be selected such that it offers the support for Availability zones. This ensures HA of the regional deployment stamps. **Note**: Not all regions in the US have the availability zones support. The standard proposes to make use of the Availability set where Availability zones are not available
   - The selected region also needs to support the services that have been chosen for this workload
   - Another important consideration is the availability of enough amount of resources in the secondary region. E.g., if your subscription does not have enough number of compute cores in the selected region, then the scale-out action of the VMSS would fail and this would affect in the failure of the requests
     - This contraint places a direct emphasis on the design practice of doing a *Resource Requirement Etimation*  and *Platform Capacity Assessment** and  and creating requests for resources that are to be approved
2. **Defining RPO and RTO** -  These have not been defined in this solution as we have not exercised a direct DR scenario in here. However, the general guidance is to define the expected RPO and RTO and see if the multi-region design is capable of supporting these requirements. **TBD**: Add information on the RPO and RTO support that Azure SQLDB Business Critical Tier provides
3. **Safe deployment**- The [Azure safe deployment practice (SDP) framework](https://azure.microsoft.com/blog/advancing-safe-deployment-practices) ensures that all code and configuration changes (planned maintenance) to the Azure platform undergo a phased rollout. 
4. **CDN, Edge-Caching** - It is a suggested practice to use the features of Content Delivery networks and edge caching if the workload is HTTP based and Application Delivery Network capabilities (i.e. accelerated delivery) are required. These are baked into Azure Front Door. This solution does not use AFD for reasons that would be elaborated in a separate section

### Constrained migrations via IaaS
This section from the documentation talks about the scenario where a workload has not been modernized yet, i.e. usage of containers or other cloud native technologies. We have taken such a scenario for our solution implementation where in the assumption is the web application runs on Virtual Machines on-premise. So the Azure based deployment stamps would also run the workload on Azure VMSS
#### Design Considerations and Recommendations
1. **Operational Costs of the VMs** - The operational costs of using IaaS virtual machines are significantly higher than the costs of using PaaS services because of the management requirements of the virtual machines and the operating systems. Managing virtual machines necessitates the frequent rollout of software packages and updates [**this is a direct excerpt from the documentation**]
2. **Availability of VMs** - To make sure the compute part of the workload is Highly Available, we have chosen the VMSS with autoscaling and the instances to be placed across the availability zones
3. **Right Sizing the VMs**- This is an important exercise to be performed when selecting the SKU of VMs that will be added to the Virtual Machine Scalesets. Azure provides a very nice tool that can simplify the job for you. The VM selection tool asks you a series of questions and then comes out with suggestions on the suitable SKUs of VMs. The output sheet from the exercise done for this solution is added [here](../Worksheets/VM-SelectionTool-OutputData.xlsx)
4. **Number of VMs for high-availability**- The general guidance is to have a minimum of 3 instances spread across the availability zones. However, in this use case, bursting to cloud has a default NFR to keep the cost low. This has been highlighted in the definition of "Burst to Cloud". So the design decision is to have just one instance in the VMSS by default and scale out by 2 instances every time a threshold is reached. 
5. **Scalability and Zone Redundancy**- VMSS in each of the Azure stamps will be created where the instances will be placed across availability zones. Scalilng will be automatic and will depend on an identified application performance threshold. The scale-out and scale-in actions need to be tested carefully so that the varying loads and sudden spikes are handled efficiently
6. **Use of load balancers** - this is a straight-forward design criteria and should be a no-brainer. We use an external load balancer that will receive the egress traffic from the client in its public frontend IP address
7. **Use of standard Images**- The solution requires preparing a custom image and maintaining that as an Azure Managed Image or as an Image in the Compute Gallery. The reason for using a custom image is that the VM scale set uses the image to create a new instance every time it scales out. If the post VM creation script is heavy and is time consuming, then the scale-out operation would be delayed. To handle this issue, we will have the a)installation of IIS, creation of a Virtual directory and website and deploying of the stable version of the website done on a VM. We then sysprep the machine to prepare a golden image per region. This image is then used to create the VMSS. This process helps in briniging down the instance creation time during a scale-out by 300-400%
8. **Monitor virtual machines** - This has not been implemented to its entirety. We however do have provisions for the infrastructure logs from the scale set to be ingested into a regional log analytics workspace. The diagnostics settings need to be added.     

## 2. Data Platform Design
### Design Considerations and Recommendations
#### Volume
1. **Data Volume Growth** - The data storage requirements should be carefully assessed when deciding the storage sizing of Azure SQLDB or the size of the data disks if using SQL on Azure Virtual Machines. This becomes as important aspect of right-sizing the components to avoid issues related to data growth scenarios. The main metric that would help in deciding the volume of data growth would be the growth pattern in the past 'N' months. The data platform should be designed to accommodate an increase in the data storage needs. This requires over-provisioning Azure SQLDB with a considerable buffer of storage space. This may sound like counter-inyuitive after having adopted the cloud which favors the "Use on demand model". As opposed to provisioning additional storage space, an automation mechanism can be setup to scale-up the storage capacity on-demand. This automation script can be executed in response to an alert that indicates exhaustion of 70% of the available space.  
*References*  
[Changing Azure SQLDB Storage Size](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-scale?view=azuresql#change-storage-size)  
[Using Powershell to Increase the Size of an Azure SQLDB](https://learn.microsoft.com/en-us/powershell/module/az.sql/set-azsqldatabase?view=azps-9.7.0#example-3-modify-the-storage-max-size-of-a-database)  
[Imapact of Scaling Azure SQL resources up/down](https://learn.microsoft.com/en-us/azure/azure-sql/database/scale-resources?view=azuresql#impact-of-scale-up-or-scale-down-operations)  
2. **Removal or Offloading of older data**- This has not been implemented in this solution. The suggestion however is to have the data that will not be used and has not been for a very long time moved to a cold storage. Backup of the Azure SQLDB databases can be a good starting point. Unused databases, and old records from databases that are in use can be good candidates for further analysis

